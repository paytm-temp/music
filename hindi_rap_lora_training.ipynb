{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Hindi Rap LoRA Finetuning\n",
        "\n",
        "This notebook demonstrates how to finetune a music generation model using LoRA (Low-Rank Adaptation) for Hindi rap generation. The model will be trained on a dataset of Hindi rap songs.\n",
        "\n",
        "## Setup and Requirements\n",
        "\n",
        "First, we'll set up our environment and install the required dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive (if needed)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/paytm-temp/music.git\n",
        "%cd music\n",
        "\n",
        "# Install dependencies\n",
        "%pip install -r requirements.txt\n",
        "%pip install torch torchaudio transformers datasets accelerate wandb\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "Now we'll prepare our dataset by loading the lyrics and audio files. The dataset consists of Hindi rap songs with their corresponding lyrics and prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from acestep.models.lyrics_utils.lyric_tokenizer import LyricTokenizer\n",
        "from acestep.models.lyrics_utils.lyric_normalizer import LyricNormalizer\n",
        "from acestep.models.lyrics_utils.lyric_encoder import LyricEncoder\n",
        "from acestep.music_dcae.music_log_mel import MusicLogMel\n",
        "\n",
        "# Initialize tokenizer and normalizer\n",
        "with open('acestep/models/lyrics_utils/vocab.json', 'r', encoding='utf-8') as f:\n",
        "    vocab = json.load(f)\n",
        "    \n",
        "tokenizer = LyricTokenizer(vocab)\n",
        "normalizer = LyricNormalizer()\n",
        "encoder = LyricEncoder()\n",
        "\n",
        "# Initialize audio processor\n",
        "audio_processor = MusicLogMel()\n",
        "\n",
        "def prepare_song_data(song_dir='data/songs'):\n",
        "    dataset = []\n",
        "    for file in os.listdir(song_dir):\n",
        "        if file.endswith('.mp3'):\n",
        "            base_name = file[:-4]\n",
        "            lyrics_file = os.path.join(song_dir, base_name + '_lyrics.txt')\n",
        "            prompt_file = os.path.join(song_dir, base_name + '_prompt.txt')\n",
        "            audio_file = os.path.join(song_dir, file)\n",
        "            \n",
        "            if os.path.exists(lyrics_file) and os.path.exists(prompt_file):\n",
        "                with open(lyrics_file, 'r', encoding='utf-8') as f:\n",
        "                    lyrics = f.read().strip()\n",
        "                with open(prompt_file, 'r', encoding='utf-8') as f:\n",
        "                    prompt = f.read().strip()\n",
        "                    \n",
        "                # Process lyrics and prompt\n",
        "                normalized_lyrics = normalizer.normalize(lyrics)\n",
        "                normalized_prompt = normalizer.normalize(prompt)\n",
        "                \n",
        "                dataset.append({\n",
        "                    'audio_path': audio_file,\n",
        "                    'lyrics': normalized_lyrics,\n",
        "                    'prompt': normalized_prompt\n",
        "                })\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "# Prepare dataset\n",
        "dataset = prepare_song_data()\n",
        "print(f\"Prepared {len(dataset)} songs for training\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Model Configuration\n",
        "\n",
        "Let's load and configure the model for LoRA training. We'll use the configuration from the project's config file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from acestep.models.ace_step_transformer import AceStepTransformer\n",
        "from acestep.schedulers.scheduling_flow_match_euler_discrete import FlowMatchEulerDiscreteScheduler\n",
        "\n",
        "# Load configuration\n",
        "with open('config/hi_rap_lora_config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Initialize model and scheduler\n",
        "model = AceStepTransformer(config)\n",
        "scheduler = FlowMatchEulerDiscreteScheduler(\n",
        "    num_train_timesteps=config.get('num_train_timesteps', 1000),\n",
        "    beta_schedule=config.get('beta_schedule', 'linear')\n",
        ")\n",
        "\n",
        "# Configure LoRA parameters\n",
        "lora_config = {\n",
        "    'r': 16,  # LoRA rank\n",
        "    'alpha': 32,  # LoRA scaling factor\n",
        "    'dropout': 0.1,\n",
        "    'target_modules': ['q_proj', 'k_proj', 'v_proj', 'out_proj']  # Layers to apply LoRA\n",
        "}\n",
        "\n",
        "# Enable LoRA training\n",
        "model.enable_lora_training(lora_config)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model initialized on {device}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Training Setup\n",
        "\n",
        "Now we'll set up the training loop with DataLoader and optimizer configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class HindiRapDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, audio_processor):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.audio_processor = audio_processor\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        \n",
        "        # Process audio\n",
        "        audio_features = self.audio_processor.process(item['audio_path'])\n",
        "        \n",
        "        # Process text\n",
        "        lyrics_tokens = self.tokenizer.tokenize(item['lyrics'])\n",
        "        prompt_tokens = self.tokenizer.tokenize(item['prompt'])\n",
        "        \n",
        "        return {\n",
        "            'audio_features': torch.tensor(audio_features, device=device),\n",
        "            'lyrics_tokens': torch.tensor(lyrics_tokens, device=device),\n",
        "            'prompt_tokens': torch.tensor(prompt_tokens, device=device)\n",
        "        }\n",
        "\n",
        "# Training parameters\n",
        "train_params = {\n",
        "    'batch_size': 4,\n",
        "    'learning_rate': 1e-4,\n",
        "    'num_epochs': 50,\n",
        "    'gradient_accumulation_steps': 2,\n",
        "    'save_steps': 500,\n",
        "    'warmup_steps': 100\n",
        "}\n",
        "\n",
        "# Create dataset and dataloader\n",
        "train_dataset = HindiRapDataset(dataset, tokenizer, audio_processor)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=train_params['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=train_params['learning_rate'],\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "print(f\"Training setup complete with {len(train_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Now we'll implement the training loop with progress tracking and model checkpointing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import wandb  # Optional: for tracking experiments\n",
        "\n",
        "# Initialize wandb (optional)\n",
        "# wandb.init(project=\"hindi-rap-lora\", config=train_params)\n",
        "\n",
        "# Create checkpoint directory\n",
        "checkpoint_dir = Path('checkpoints/hindi_rap_lora')\n",
        "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "global_step = 0\n",
        "model.train()\n",
        "\n",
        "for epoch in range(train_params['num_epochs']):\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Forward pass\n",
        "        loss = model(\n",
        "            audio_features=batch['audio_features'],\n",
        "            lyrics_tokens=batch['lyrics_tokens'],\n",
        "            prompt_tokens=batch['prompt_tokens']\n",
        "        )\n",
        "        \n",
        "        # Backward pass\n",
        "        loss = loss / train_params['gradient_accumulation_steps']\n",
        "        loss.backward()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        # Update weights if gradient accumulation is complete\n",
        "        if (batch_idx + 1) % train_params['gradient_accumulation_steps'] == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            \n",
        "            # Log metrics\n",
        "            avg_loss = epoch_loss / (batch_idx + 1)\n",
        "            progress_bar.set_postfix({'loss': avg_loss})\n",
        "            # wandb.log({'loss': avg_loss, 'epoch': epoch, 'step': global_step})\n",
        "            \n",
        "            # Save checkpoint\n",
        "            if global_step % train_params['save_steps'] == 0:\n",
        "                checkpoint_path = checkpoint_dir / f'checkpoint-{global_step}.pt'\n",
        "                torch.save({\n",
        "                    'step': global_step,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': avg_loss,\n",
        "                    'epoch': epoch,\n",
        "                }, checkpoint_path)\n",
        "                print(f\"\\nSaved checkpoint: {checkpoint_path}\")\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Save LoRA Weights\n",
        "\n",
        "Finally, let's save the trained LoRA weights for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final LoRA weights\n",
        "final_weights_path = checkpoint_dir / 'final_lora_weights.pt'\n",
        "model.save_lora_weights(final_weights_path)\n",
        "print(f\"Saved final LoRA weights to: {final_weights_path}\")\n",
        "\n",
        "# Save training configuration\n",
        "config_path = checkpoint_dir / 'training_config.json'\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump({\n",
        "        'train_params': train_params,\n",
        "        'lora_config': lora_config\n",
        "    }, f, indent=2)\n",
        "print(f\"Saved training configuration to: {config_path}\")\n",
        "\n",
        "# Optional: Save to Google Drive\n",
        "# drive_path = '/content/drive/MyDrive/hindi_rap_lora'\n",
        "# !mkdir -p {drive_path}\n",
        "# !cp -r {checkpoint_dir}/* {drive_path}/\n",
        "# print(f\"Copied weights to Google Drive: {drive_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
