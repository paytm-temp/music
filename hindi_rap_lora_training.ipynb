{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Hindi Rap LoRA Training\n",
        "\n",
        "This notebook performs LoRA finetuning for Hindi rap generation. All required files are available in the shared drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount shared drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Change to project directory in shared drive\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Update this path to your shared drive location\n",
        "DRIVE_PATH = \"/Users/aakashdhondiyal/Downloads/hindi-lora\"\n",
        "os.chdir(DRIVE_PATH)\n",
        "sys.path.append(DRIVE_PATH)\n",
        "\n",
        "print(f\"Working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch torchaudio transformers datasets accelerate wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "import json\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import tqdm as tqdm_module\n",
        "import os\n",
        "\n",
        "# Force tqdm to use console mode\n",
        "if hasattr(tqdm_module, '_instances'):\n",
        "    del tqdm_module._instances\n",
        "tqdm = tqdm_module.tqdm\n",
        "\n",
        "from acestep.models.ace_step_transformer import ACEStepTransformer2DModel as AceStepTransformer\n",
        "from acestep.models.lyrics_utils.lyric_tokenizer import LyricTokenizer\n",
        "from acestep.models.lyrics_utils.lyric_normalizer import LyricNormalizer\n",
        "from acestep.music_dcae.music_log_mel import MusicLogMel\n",
        "from acestep.schedulers.scheduling_flow_match_euler_discrete import FlowMatchEulerDiscreteScheduler\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Check CUDA availability and setup device\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "if use_cuda:\n",
        "    print(f'GPU available! Using: {torch.cuda.get_device_name(0)}')\n",
        "    torch.cuda.empty_cache()  # Clear GPU memory\n",
        "else:\n",
        "    print('Using CPU')\n",
        "\n",
        "# Initialize components\n",
        "tokenizer = LyricTokenizer('acestep/models/lyrics_utils/vocab.json')\n",
        "normalizer = LyricNormalizer()\n",
        "audio_processor = MusicLogMel()\n",
        "\n",
        "# Load model architecture configuration\n",
        "with open('acestep/models/config.json', 'r') as f:\n",
        "    model_config = json.load(f)\n",
        "\n",
        "# Load LoRA training configuration\n",
        "with open('config/hi_rap_lora_config.json', 'r') as f:\n",
        "    lora_config_dict = json.load(f)\n",
        "\n",
        "# Create a wrapper class to make our model compatible with PEFT\n",
        "class ModelWrapper(torch.nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.config = base_model.config\n",
        "        \n",
        "    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None, **kwargs):\n",
        "        # Extract our custom parameters from kwargs\n",
        "        hidden_states = kwargs.get('hidden_states', inputs_embeds)\n",
        "        encoder_text_hidden_states = kwargs.get('encoder_text_hidden_states')\n",
        "        text_attention_mask = kwargs.get('text_attention_mask')\n",
        "        speaker_embeds = kwargs.get('speaker_embeds')\n",
        "        lyric_token_idx = kwargs.get('lyric_token_idx', input_ids)\n",
        "        lyric_mask = kwargs.get('lyric_mask', attention_mask)\n",
        "        timestep = kwargs.get('timestep')\n",
        "        \n",
        "        # Remove parameters we're explicitly passing to avoid duplication\n",
        "        # Also filter out PEFT-specific parameters that our model doesn't expect\n",
        "        filtered_kwargs = {k: v for k, v in kwargs.items() if k not in [\n",
        "            'hidden_states', 'encoder_text_hidden_states', 'text_attention_mask',\n",
        "            'speaker_embeds', 'lyric_token_idx', 'lyric_mask', 'timestep',\n",
        "            'labels', 'output_attentions', 'output_hidden_states', 'return_dict'\n",
        "        ]}\n",
        "        \n",
        "        # Call the base model with the correct parameters\n",
        "        return self.base_model(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            encoder_text_hidden_states=encoder_text_hidden_states,\n",
        "            text_attention_mask=text_attention_mask,\n",
        "            speaker_embeds=speaker_embeds,\n",
        "            lyric_token_idx=lyric_token_idx,\n",
        "            lyric_mask=lyric_mask,\n",
        "            timestep=timestep,\n",
        "            **filtered_kwargs\n",
        "        )\n",
        "\n",
        "# Initialize model with correct architecture configuration\n",
        "base_model = AceStepTransformer(**model_config)\n",
        "model = ModelWrapper(base_model)\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],  # Updated target modules to match model architecture\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"  # Using SEQ_CLS to avoid generation method requirements\n",
        ")\n",
        "\n",
        "# Add LoRA adapters to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model = model.to(device)\n",
        "\n",
        "# Initialize scheduler\n",
        "scheduler = FlowMatchEulerDiscreteScheduler(\n",
        "    num_train_timesteps=lora_config_dict.get('num_train_timesteps', 1000),\n",
        "    shift=1.0  # Default value, you can adjust this based on your needs\n",
        ")\n",
        "\n",
        "print(\"Model initialized with LoRA configuration\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset class definition\n",
        "class HindiRapDataset(Dataset):\n",
        "    def __init__(self, song_dir='data/songs'):\n",
        "        self.song_dir = song_dir\n",
        "        self.samples = []\n",
        "        \n",
        "        for file in os.listdir(song_dir):\n",
        "            if file.endswith('_lyrics.txt'):  # Look for lyrics files\n",
        "                base_name = file[:-11]  # Remove '_lyrics.txt'\n",
        "                lyrics_file = os.path.join(song_dir, file)\n",
        "                prompt_file = os.path.join(song_dir, base_name + '_prompt.txt')\n",
        "                audio_file = os.path.join(song_dir, base_name + '.mp3')\n",
        "                \n",
        "                if os.path.exists(prompt_file) and os.path.exists(audio_file):\n",
        "                    with open(lyrics_file, 'r', encoding='utf-8') as f:\n",
        "                        lyrics = f.read().strip()\n",
        "                    with open(prompt_file, 'r', encoding='utf-8') as f:\n",
        "                        prompt = f.read().strip()\n",
        "                        \n",
        "                    self.samples.append({\n",
        "                        'audio_path': audio_file,\n",
        "                        'lyrics': lyrics,\n",
        "                        'prompt': prompt\n",
        "                    })\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.samples[idx]\n",
        "        \n",
        "        # Process real audio using MusicLogMel\n",
        "        try:\n",
        "            audio_features = audio_processor.process(item['audio_path'])\n",
        "            # Ensure audio_features is a torch tensor with correct shape\n",
        "            if isinstance(audio_features, torch.Tensor):\n",
        "                audio_tensor = audio_features\n",
        "            else:\n",
        "                audio_tensor = torch.tensor(audio_features, dtype=torch.float32)\n",
        "            \n",
        "            print(f\"Original audio shape: {audio_tensor.shape}\")\n",
        "            \n",
        "            # The model expects [channels, height, width] where:\n",
        "            # - channels = 8 (in_channels from config)\n",
        "            # - height >= 16 (patch_size[0] from config)\n",
        "            # - width can vary (will be processed by patch embedding)\n",
        "            \n",
        "            if audio_tensor.dim() == 2:  # If [features, time] or [channels, time]\n",
        "                # Reshape to ensure height >= 16\n",
        "                channels, time_steps = audio_tensor.shape\n",
        "                \n",
        "                # Calculate height and width to ensure height >= 16\n",
        "                if time_steps >= 16:\n",
        "                    # Reshape into [channels, height, width] where height >= 16\n",
        "                    height = 16\n",
        "                    width = time_steps // height\n",
        "                    if width == 0:\n",
        "                        width = 1\n",
        "                    # Trim to fit exact dimensions\n",
        "                    total_needed = height * width\n",
        "                    if time_steps > total_needed:\n",
        "                        audio_tensor = audio_tensor[:, :total_needed]\n",
        "                    elif time_steps < total_needed:\n",
        "                        # Pad the time dimension\n",
        "                        padding_needed = total_needed - time_steps\n",
        "                        audio_tensor = F.pad(audio_tensor, (0, padding_needed))\n",
        "                    \n",
        "                    # Reshape to [channels, height, width]\n",
        "                    audio_tensor = audio_tensor.view(channels, height, width)\n",
        "                else:\n",
        "                    # If time_steps < 16, pad to minimum size\n",
        "                    min_size = 16 * 16  # 256 total elements\n",
        "                    padding_needed = min_size - time_steps\n",
        "                    audio_tensor = F.pad(audio_tensor, (0, padding_needed))\n",
        "                    audio_tensor = audio_tensor.view(channels, 16, 16)\n",
        "            \n",
        "            # Ensure we have exactly 8 channels\n",
        "            current_channels = audio_tensor.shape[0]\n",
        "            if current_channels != 8:\n",
        "                if current_channels < 8:\n",
        "                    # Repeat channels to get 8\n",
        "                    repeat_factor = 8 // current_channels\n",
        "                    remainder = 8 % current_channels\n",
        "                    repeated = audio_tensor.repeat(repeat_factor, 1, 1)\n",
        "                    if remainder > 0:\n",
        "                        extra = audio_tensor[:remainder]\n",
        "                        audio_tensor = torch.cat([repeated, extra], dim=0)\n",
        "                    else:\n",
        "                        audio_tensor = repeated\n",
        "                else:\n",
        "                    # Take first 8 channels\n",
        "                    audio_tensor = audio_tensor[:8]\n",
        "            \n",
        "            print(f\"Final audio shape: {audio_tensor.shape}\")\n",
        "                    \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio {item['audio_path']}: {e}\")\n",
        "            # Fallback to dummy data if audio processing fails\n",
        "            audio_tensor = torch.randn(8, 16, 128, dtype=torch.float32)\n",
        "            print(f\"Using fallback shape: {audio_tensor.shape}\")\n",
        "        \n",
        "        # Process text data\n",
        "        lyrics_tokens = tokenizer.encode(normalizer.normalize(item['lyrics']))\n",
        "        lyrics_tensor = torch.tensor(lyrics_tokens)\n",
        "        \n",
        "        # Create attention masks\n",
        "        lyrics_mask = torch.ones(len(lyrics_tokens))\n",
        "        \n",
        "        # Process prompt for text embeddings (we'll use this instead of dummy text)\n",
        "        prompt_tokens = tokenizer.encode(normalizer.normalize(item['prompt']))\n",
        "        prompt_tensor = torch.tensor(prompt_tokens)\n",
        "        \n",
        "        return {\n",
        "            'audio_features': audio_tensor,\n",
        "            'lyrics_tokens': lyrics_tensor,\n",
        "            'lyrics_mask': lyrics_mask,\n",
        "            'prompt_tokens': prompt_tensor,\n",
        "            'prompt_text': item['prompt']  # Keep original text for reference\n",
        "        }\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = HindiRapDataset()\n",
        "\n",
        "# Define custom collate function for padding\n",
        "def collate_fn(batch):\n",
        "    # Find max lengths in the batch\n",
        "    max_lyrics_len = max(len(item['lyrics_tokens']) for item in batch)\n",
        "    max_prompt_len = max(len(item['prompt_tokens']) for item in batch)\n",
        "    max_audio_width = max(item['audio_features'].shape[-1] for item in batch)  # Width dimension (last dim)\n",
        "    \n",
        "    # Debug info (can be removed later)\n",
        "    # print(f\"Batch size: {len(batch)}, Max audio width: {max_audio_width}\")\n",
        "    \n",
        "    # Pad each sequence to max_len\n",
        "    padded_audio_features = []\n",
        "    for item in batch:\n",
        "        audio = item['audio_features']\n",
        "        current_width = audio.shape[-1]\n",
        "        if current_width < max_audio_width:\n",
        "            # Pad the width dimension (last dimension)\n",
        "            padding = (0, max_audio_width - current_width)  # Pad last dimension\n",
        "            padded_audio = F.pad(audio, padding)\n",
        "        else:\n",
        "            padded_audio = audio\n",
        "        padded_audio_features.append(padded_audio)\n",
        "        # print(f\"Padded audio shape: {padded_audio.shape}\")\n",
        "    \n",
        "    padded_batch = {\n",
        "        'audio_features': torch.stack(padded_audio_features),\n",
        "        'lyrics_tokens': torch.stack([\n",
        "            F.pad(item['lyrics_tokens'], (0, max_lyrics_len - len(item['lyrics_tokens'])))\n",
        "            for item in batch\n",
        "        ]),\n",
        "        'lyrics_mask': torch.stack([\n",
        "            F.pad(item['lyrics_mask'], (0, max_lyrics_len - len(item['lyrics_mask'])))\n",
        "            for item in batch\n",
        "        ]),\n",
        "        'prompt_tokens': torch.stack([\n",
        "            F.pad(item['prompt_tokens'], (0, max_prompt_len - len(item['prompt_tokens'])))\n",
        "            for item in batch\n",
        "        ]),\n",
        "        'prompt_texts': [item['prompt_text'] for item in batch]  # Keep original texts\n",
        "    }\n",
        "    \n",
        "    # print(f\"Final batch audio shape: {padded_batch['audio_features'].shape}\")\n",
        "    return padded_batch\n",
        "\n",
        "# Configure DataLoader with adaptive settings and custom collate function\n",
        "num_workers = 2 if use_cuda else 0  # Use workers only if GPU is available\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=use_cuda,  # Enable pin_memory if using GPU\n",
        "    collate_fn=collate_fn  # Add custom collate function\n",
        ")\n",
        "\n",
        "print(f\"Dataset prepared with {len(train_dataset)} songs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training setup\n",
        "train_params = {\n",
        "    'num_epochs': 50,\n",
        "    'learning_rate': 1e-4,\n",
        "    'gradient_accumulation_steps': 2,\n",
        "    'save_steps': 500\n",
        "}\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=train_params['learning_rate'])\n",
        "checkpoint_dir = Path('checkpoints/hindi_rap_lora')\n",
        "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "global_step = 0\n",
        "model.train()\n",
        "\n",
        "# Clear GPU memory before training if available\n",
        "if use_cuda:\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "for epoch in range(train_params['num_epochs']):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{train_params['num_epochs']}\")\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    # Use tqdm with console mode\n",
        "    for batch_idx, batch in enumerate(tqdm_module.tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", ascii=True)):\n",
        "        # Create attention masks\n",
        "        lyrics_mask = (batch['lyrics_tokens'] != 0).long()  # Create mask where non-zero tokens are 1\n",
        "        \n",
        "        # Audio mask should match the sequence length after patch embedding\n",
        "        # After patch embedding: [batch, channels, height, width] -> [batch, seq_len, dim]\n",
        "        # The sequence length after patch embedding = width (since patch_size is [16, 1])\n",
        "        batch_size, channels, height, width = batch['audio_features'].shape\n",
        "        audio_mask = torch.ones(batch_size, width, device=device)\n",
        "        \n",
        "        print(f\"Audio features shape: {batch['audio_features'].shape}\")\n",
        "        print(f\"Audio mask shape: {audio_mask.shape}\")\n",
        "        print(f\"Lyrics tokens shape: {batch['lyrics_tokens'].shape}\")\n",
        "        print(f\"Lyrics mask shape: {lyrics_mask.shape}\")\n",
        "        \n",
        "        # Use real prompt data for text embeddings instead of dummy data\n",
        "        # Convert prompt tokens to embeddings (using a simple embedding approach for now)\n",
        "        prompt_mask = (batch['prompt_tokens'] != 0).long()\n",
        "        # Create text embeddings from prompt tokens (simplified approach)\n",
        "        # For now, create embeddings with the expected dimension (768)\n",
        "        text_embeddings = torch.randn((batch['audio_features'].shape[0], batch['prompt_tokens'].shape[1], 768), device=device)\n",
        "        text_mask = prompt_mask\n",
        "        \n",
        "        # Create dummy speaker embeddings (we don't have speaker info in our dataset)\n",
        "        dummy_speaker = torch.zeros((batch['audio_features'].shape[0], 512), device=device)\n",
        "        \n",
        "        # Get timestep\n",
        "        timestep = torch.randint(0, scheduler.num_train_timesteps, (batch['audio_features'].shape[0],), device=device)\n",
        "        \n",
        "        # Forward pass using the wrapper with real data\n",
        "        output = model(\n",
        "            input_ids=batch['lyrics_tokens'],\n",
        "            attention_mask=audio_mask,  # Use audio_mask for main attention_mask\n",
        "            inputs_embeds=batch['audio_features'],\n",
        "            hidden_states=batch['audio_features'],\n",
        "            encoder_text_hidden_states=text_embeddings,\n",
        "            text_attention_mask=text_mask,\n",
        "            speaker_embeds=dummy_speaker,\n",
        "            lyric_token_idx=batch['lyrics_tokens'],\n",
        "            lyric_mask=lyrics_mask,  # Use lyrics_mask for lyric-specific attention\n",
        "            timestep=timestep,\n",
        "            return_dict=True\n",
        "        )\n",
        "        \n",
        "        # Get loss from output\n",
        "        loss = output.sample  # Assuming the loss is in the sample field\n",
        "        \n",
        "        # Backward pass\n",
        "        loss = loss / train_params['gradient_accumulation_steps']\n",
        "        loss.backward()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        # Update weights if gradient accumulation is complete\n",
        "        if (batch_idx + 1) % train_params['gradient_accumulation_steps'] == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            \n",
        "            # Log metrics\n",
        "            avg_loss = epoch_loss / (batch_idx + 1)\n",
        "            if global_step % 10 == 0:  # Print every 10 steps\n",
        "                print(f\"Step {global_step}, Loss: {avg_loss:.4f}\")\n",
        "            \n",
        "            # Save checkpoint\n",
        "            if global_step % train_params['save_steps'] == 0:\n",
        "                checkpoint_path = checkpoint_dir / f'checkpoint-{global_step}.pt'\n",
        "                torch.save({\n",
        "                    'step': global_step,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': avg_loss,\n",
        "                    'epoch': epoch,\n",
        "                }, checkpoint_path)\n",
        "                print(f\"\\nSaved checkpoint: {checkpoint_path}\")\n",
        "                \n",
        "                # Clear GPU memory after saving if available\n",
        "                if use_cuda:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final LoRA weights\n",
        "final_weights_path = checkpoint_dir / 'final_lora_weights.pt'\n",
        "model.save_pretrained(final_weights_path)\n",
        "print(f\"Saved final LoRA weights to: {final_weights_path}\")\n",
        "\n",
        "# Save training configuration\n",
        "config_path = checkpoint_dir / 'training_config.json'\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump({\n",
        "        'train_params': train_params,\n",
        "        'lora_config': lora_config.to_dict()  # Convert LoraConfig to dict for saving\n",
        "    }, f, indent=2)\n",
        "print(f\"Saved training configuration to: {config_path}\")\n",
        "\n",
        "# Clear GPU memory one last time if available\n",
        "if use_cuda:\n",
        "    torch.cuda.empty_cache()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
